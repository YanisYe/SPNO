# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from functools import lru_cache
from re import X

import numpy as np
import torch
import torch.nn as nn
from timm.models.vision_transformer import Block, PatchEmbed, trunc_normal_,DropPath,Mlp, Attention
import sys
# from climax.parallelpatchembed import ParallelVarPatchEmbed
import torch.nn.functional as F
from torch_harmonics import NeighborhoodAttentionS2
sys.path.append("/home/hunter/workspace/climate/climate_predict/")
from sphere_conv import SphereConv2d
from torch_harmonics.examples.models._layers import MLP, DropPath, LayerNorm, SequencePositionEmbedding, SpectralPositionEmbedding, LearnablePositionEmbedding

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Tuple, Optional, Union


def latlon_to_xyz(coords, radius=1.0):
    """
    å°†ç»çº¬åº¦è½¬æ¢ä¸º 3D ç¬›å¡å°”åæ ‡ï¼ˆå‚è€ƒ sphere_posEmb.pyï¼‰
    è¿™æ ·å¯ä»¥é¿å…åœ¨ç»çº¬åº¦ç©ºé—´ä¸­ç›´æ¥è®¡ç®—è·ç¦»çš„é—®é¢˜ï¼ˆæç‚¹å¥‡å¼‚æ€§ã€ç»åº¦å‘¨æœŸæ€§ç­‰ï¼‰
    
    Args:
        coords: [..., 2] -> (lat, lon) å¼§åº¦
        radius: çƒé¢åŠå¾„ï¼Œé»˜è®¤ä¸º 1.0
    
    Returns:
        xyz: [..., 3] -> (x, y, z) 3D ç¬›å¡å°”åæ ‡
    """
    lat = coords[..., 0]
    lon = coords[..., 1]
    # æ°”è±¡ä¹ æƒ¯ï¼šlat ä» pi/2 åˆ° -pi/2
    x = radius * torch.cos(lat) * torch.cos(lon)
    y = radius * torch.cos(lat) * torch.sin(lon)
    z = radius * torch.sin(lat)
    return torch.stack([x, y, z], dim=-1)


def compute_geodesic_distance(coords1, coords2, radius=1.0, coords_in_degrees=False):
    """
    è®¡ç®—çƒé¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„æµ‹åº¦è·ç¦»ï¼ˆgeodesic distanceï¼‰
    å‚è€ƒ sphere_posEmb.py çš„æ–¹æ³•ï¼šå…ˆå°†ç»çº¬åº¦è½¬æ¢ä¸º3Dç¬›å¡å°”åæ ‡ï¼Œç„¶åè®¡ç®—3Dç©ºé—´ä¸­çš„è·ç¦»
    
    Args:
        coords1: [N, 2] ç¬¬ä¸€ç»„åæ ‡ (lat, lon)
        coords2: [M, 2] ç¬¬äºŒç»„åæ ‡ (lat, lon)
        radius: çƒé¢åŠå¾„ï¼Œé»˜è®¤ä¸º 1.0ï¼ˆå•ä½çƒé¢ï¼‰
        coords_in_degrees: bool, å¦‚æœä¸º Trueï¼Œåæ ‡å•ä½ä¸ºåº¦æ•°ï¼Œéœ€è¦è½¬æ¢ä¸ºå¼§åº¦
    
    Returns:
        distances: [N, M] è·ç¦»çŸ©é˜µï¼Œå•ä½ä¸ radius ç›¸åŒ
    """
    # å¦‚æœéœ€è¦ï¼Œå°†åº¦æ•°è½¬æ¢ä¸ºå¼§åº¦
    if coords_in_degrees:
        coords1 = torch.deg2rad(coords1)
        coords2 = torch.deg2rad(coords2)
    
    # è½¬æ¢ä¸º3Dç¬›å¡å°”åæ ‡
    coords1_3d = latlon_to_xyz(coords1, radius=radius)  # [N, 3]
    coords2_3d = latlon_to_xyz(coords2, radius=radius)  # [M, 3]
    
    # åœ¨3Dç©ºé—´ä¸­è®¡ç®—æ¬§å¼è·ç¦»ï¼ˆè¿™å°±æ˜¯çƒé¢ä¸Šçš„æµ‹åº¦è·ç¦»ï¼‰
    # (N, 1, 3) ä¸ (1, M, 3) å¹¿æ’­ -> (N, M, 3) -> (N, M)
    d2 = ((coords1_3d[:, None, :] - coords2_3d[None, :, :]) ** 2).sum(dim=2)
    distances = torch.sqrt(d2)
    
    return distances


def compute_geodesic_distance_squared(coords1, coords2, radius=1.0, coords_in_degrees=False):
    """
    è®¡ç®—çƒé¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„æµ‹åº¦è·ç¦»çš„å¹³æ–¹
    å‚è€ƒ sphere_posEmb.py çš„æ–¹æ³•ï¼šä½¿ç”¨3Dç¬›å¡å°”åæ ‡è®¡ç®—
    
    Args:
        coords1: [N, 2] ç¬¬ä¸€ç»„åæ ‡ (lat, lon)
        coords2: [M, 2] ç¬¬äºŒç»„åæ ‡ (lat, lon)
        radius: çƒé¢åŠå¾„ï¼Œé»˜è®¤ä¸º 1.0
        coords_in_degrees: bool, å¦‚æœä¸º Trueï¼Œåæ ‡å•ä½ä¸ºåº¦æ•°
    
    Returns:
        distances_squared: [N, M] è·ç¦»å¹³æ–¹çŸ©é˜µ
    """
    # å¦‚æœéœ€è¦ï¼Œå°†åº¦æ•°è½¬æ¢ä¸ºå¼§åº¦
    if coords_in_degrees:
        coords1 = torch.deg2rad(coords1)
        coords2 = torch.deg2rad(coords2)
    
    # è½¬æ¢ä¸º3Dç¬›å¡å°”åæ ‡
    coords1_3d = latlon_to_xyz(coords1, radius=radius)  # [N, 3]
    coords2_3d = latlon_to_xyz(coords2, radius=radius)  # [M, 3]
    
    # åœ¨3Dç©ºé—´ä¸­è®¡ç®—æ¬§å¼è·ç¦»çš„å¹³æ–¹ï¼ˆè¿™å°±æ˜¯çƒé¢ä¸Šæµ‹åº¦è·ç¦»çš„å¹³æ–¹ï¼‰
    # (N, 1, 3) ä¸ (1, M, 3) å¹¿æ’­ -> (N, M, 3) -> (N, M)
    d2 = ((coords1_3d[:, None, :] - coords2_3d[None, :, :]) ** 2).sum(dim=2)
    
    return d2


class SphericalCrossOperatorS2(nn.Module):
    """
    Cross Attention Operator using Spherical Neighborhood Attention and Spectral PE.
    Input/Output interface remains [B, N, D] for compatibility.
    """
    def __init__(
        self,
        dim,
        num_heads,
        in_shape=(32, 64),   # KV shape (Low Res)
        out_shape=(64, 128), # Q shape (High Res)
        mlp_ratio=4.0,
        num_layers=1,
        norm_layer=nn.LayerNorm,
        theta_cutoff=None,    # Optional: override neighborhood radius
        use_spherical_attn=True  # True: NeighborhoodAttentionS2, False: MultiheadAttention
    ):
        super().__init__()
        
        self.dim = dim
        self.in_shape = in_shape
        self.out_shape = out_shape
        self.num_heads = num_heads
        self.use_spherical_attn = use_spherical_attn

        self.layers = nn.ModuleList()

        for _ in range(num_layers):
            if use_spherical_attn:
                # S2 Neighborhood Attention
                attn_module = NeighborhoodAttentionS2(
                    in_channels=dim,
                    in_shape=in_shape,    # Low Res Grid
                    out_shape=out_shape,  # High Res Grid
                    num_heads=num_heads,
                    grid_in="equiangular",
                    grid_out="equiangular",
                    theta_cutoff=theta_cutoff,
                    bias=True,
                    # k_channels/out_channels é»˜è®¤ç­‰äº in_channels
                )
            else:
                # Standard Multihead Attention
                attn_module = nn.MultiheadAttention(
                    embed_dim=dim,
                    num_heads=num_heads,
                    bias=True,
                    batch_first=False,  # ä½¿ç”¨ (seq_len, batch, embed_dim) æ ¼å¼
                )
            
            layer = nn.ModuleDict({
                "norm_q": norm_layer(dim),
                "norm_kv": norm_layer(dim),
                "attn": attn_module,
                "norm_mlp": norm_layer(dim),
                "mlp": Mlp(
                    in_features=dim,
                    hidden_features=int(dim * mlp_ratio),
                )
            })
            self.layers.append(layer)
      
    def forward(self, query, tokens):
        """
        query:  [B, H*W, D] (High Res Target)
        tokens: [B, h*w, D] (Low Res Source)
        """
        B, N_q, D = query.shape
        B, N_kv, D = tokens.shape
        
        # æ ¡éªŒå½¢çŠ¶
        H_q, W_q = self.out_shape
        h_kv, w_kv = self.in_shape
        if self.use_spherical_attn:
            assert N_q == H_q * W_q, f"Query len {N_q} mismatch with shape {self.out_shape}"
        # assert N_kv == h_kv * w_kv , f"Token len {N_kv} mismatch with shape {self.in_shape}"

        # # -------------------------------------------------------
        # # 1. æ³¨å…¥ Spectral Position Embedding
        # # -------------------------------------------------------
        # # PE æ¨¡å—å·²ç»å¤„ç†äº† [B, N, D] çš„è¾“å…¥æƒ…å†µ
        # query = self.pos_embed_q(query)   # [B, H*W, D]
        # tokens = self.pos_embed_kv(tokens) # [B, h*w, D]

        for layer in self.layers:
            # ---------------------------------------------------
            # 2. Attention Block (Norm -> Reshape -> Attn -> Add)
            # ---------------------------------------------------
            
            # Pre-Norm
            q_norm = layer["norm_q"](query)    # [B, Nq, D]
            kv_norm = layer["norm_kv"](tokens) # [B, Nkv, D]
            
            if self.use_spherical_attn:
                # Reshape to Spatial [B, D, H, W] for NeighborhoodAttentionS2
                # transpose(1, 2) -> [B, D, N] -> view -> [B, D, H, W]
                q_spatial = q_norm.transpose(1, 2).view(B, D, H_q, W_q)
                k_spatial = kv_norm.transpose(1, 2).view(B, D, h_kv, w_kv)
                v_spatial = k_spatial # Use same features for Key and Value
                
                # Neighborhood Cross Attention
                # Output will be [B, D, H_q, W_q]
                attn_out_spatial = layer["attn"](
                    query=q_spatial,
                    key=k_spatial,
                    value=v_spatial
                )
                
                # Reshape back to Sequence [B, Nq, D]
                attn_out = attn_out_spatial.view(B, D, N_q).transpose(1, 2)
            else:
                # Standard Multihead Attention
                # MultiheadAttention expects (seq_len, batch, embed_dim) format
                # q_norm: [B, Nq, D] -> [Nq, B, D]
                # kv_norm: [B, Nkv, D] -> [Nkv, B, D]
                q_seq = q_norm.transpose(0, 1)  # [Nq, B, D]
                kv_seq = kv_norm.transpose(0, 1)  # [Nkv, B, D]
                
                # Multihead Attention
                attn_out_seq, _ = layer["attn"](
                    query=q_seq,
                    key=kv_seq,
                    value=kv_seq,
                    need_weights=False
                )
                
                # Reshape back to [B, Nq, D]
                attn_out = attn_out_seq.transpose(0, 1)  # [B, Nq, D]
            
            # Residual Connection
            query = query + attn_out
            
            # ---------------------------------------------------
            # 3. MLP Block (Norm -> MLP -> Add)
            # ---------------------------------------------------
            # MLP usually works on [B, N, D] naturally
            q_norm_mlp = layer["norm_mlp"](query)
            query = query + layer["mlp"](q_norm_mlp)

        return query

         
class SphericalFFN(nn.Module):
    """
    Spherical Geometry-aware Feed-Forward Network (SG-FFN)

    Acts as a geometry-consistent replacement for the token-wise MLP
    in Transformer blocks operating on spherical grids.
    """

    def __init__(
        self,
        dim,
        mlp_ratio=4.0,
        kernel_size=3,
        drop_path=0.0,
        act_layer=nn.GELU,
        use_layerscale=True,
        layerscale_init=1e-6,
    ):
        super().__init__()
        hidden_dim = int(dim * mlp_ratio)

        # 1) Depthwise spherical convolution (spatial mixing)
        self.dwconv = SphereConv2d(
            dim,
            dim,
            kernel_size=kernel_size,
            padding=kernel_size // 2,
            groups=dim,
        )

        # 2) Channel expansion + projection (channel mixing)
        self.pwconv1 = nn.Conv2d(dim, hidden_dim, kernel_size=1)
        self.act = act_layer()
        self.pwconv2 = nn.Conv2d(hidden_dim, dim, kernel_size=1)

        # 3) LayerScale (optional but recommended)
        if use_layerscale:
            self.gamma = nn.Parameter(layerscale_init * torch.ones(dim))
        else:
            self.gamma = None

    def forward(self, x):
        """
        x: [B, H, W, D]
        """
        # [B, H, W, D] -> [B, D, H, W]
        x = x.permute(0, 3, 1, 2)

        x = self.dwconv(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)

        # [B, D, H, W] -> [B, H, W, D]
        x = x.permute(0, 2, 3, 1)

        if self.gamma is not None:
            x = self.gamma * x

        return x


class SpatialNorm(nn.Module):
    def __init__(self, eps=1e-6):
        super().__init__()
        self.eps = eps

    def forward(self, x):
        # x: [B, H, W, D]
        mean = x.mean(dim=(1, 2), keepdim=True)
        std = x.std(dim=(1, 2), keepdim=True)
        return (x - mean) / (std + self.eps)

class SphericalSparseOperator(nn.Module):
    """
    çƒé¢ç¨€ç–æ“ä½œç¬¦ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šå±‚å¤„ç†ã€ç»´åº¦å˜æ¢å’Œçƒé¢æ³¨æ„åŠ›
    """
    def __init__(
        self, 
        dim, 
        num_heads, 
        out_dim=None,
        in_shape=None,
        out_shape=None,
        mlp_ratio=4.0, 
        num_layers=2, 
        drop_path=0.1, 
        norm_layer=nn.LayerNorm,
        grid_in="equiangular",
        grid_out="equiangular",
        chunk_size=10,
        psi_col_idx=None,
        psi_roff_idx=None,
        attn_scale_init=0.1,
    ):
        super().__init__()
        self.dim = dim
        self.out_dim = out_dim if out_dim is not None else dim
        self.in_shape = in_shape
        self.out_shape = out_shape if out_shape is not None else in_shape
        self.num_layers = num_layers
        self.chunk_size = chunk_size
        self.psi_col_idx = psi_col_idx
        self.psi_roff_idx = psi_roff_idx
        self.attn_scale_init = attn_scale_init

        # æ„å»ºå¤šå±‚çƒé¢æ³¨æ„åŠ› + MLP ç»“æ„
        if self.in_shape is not None and self.out_shape is not None:
            # ä½¿ç”¨çƒé¢æ³¨æ„åŠ›
            dpr = [x.item() for x in torch.linspace(0, drop_path, num_layers)] if num_layers > 1 else [drop_path]
            
            self.sparse_attns = nn.ModuleList()
            # self.gate_attn = nn.ParameterList()
            # self.gate_ffn = nn.ParameterList()
            for i in range(num_layers):
                self.sparse_attns.append(nn.ModuleDict({
                    "norm1": norm_layer(dim),
                    "attn": NeighborhoodAttentionS2(
                        in_channels=dim,
                        num_heads=num_heads,
                        in_shape=self.in_shape,
                        out_shape=self.out_shape,
                        grid_in=grid_in,
                        grid_out=grid_out,
                    ),
                    "norm2": norm_layer(dim),
                    "ffn": SphericalFFN(
                        dim,
                        mlp_ratio=mlp_ratio,
                        kernel_size=3,
                        drop_path=dpr[i],
                    ),
                }))
                # self.gate_attn.append(
                #     nn.Parameter(torch.logit(torch.tensor(0.5)))
                # )
                # self.gate_ffn.append(
                #     nn.Parameter(torch.logit(torch.tensor(0.5)))
                # )
       
        self.ffn = myMlp(num_layers=num_layers, hidden_dim=dim, out_dim=dim)
        
        # èåˆå‰çš„æ ‡å‡†åŒ–å±‚ï¼ˆå¯é€‰ï¼Œç”¨äºç¨³å®šè®­ç»ƒï¼‰
        self.fusion_norm_mlp = norm_layer(dim)
        self.fusion_norm_attn = norm_layer(dim)

        # ğŸ”‘ å…³é”®ï¼šattention æ˜¯"æ…¢å˜é‡"
        self.register_buffer(
            "attn_scale",
            torch.tensor(self.attn_scale_init),
            persistent=False
        )

        # è¾“å‡ºæŠ•å½±å±‚
        if self.out_dim != dim:
            self.proj_out = myMlp(num_layers=num_layers, hidden_dim=dim, out_dim=self.out_dim)
        else:
            self.proj_out = nn.Identity()
        
        self.use_spherical_attn = (self.in_shape is not None and self.out_shape is not None)
    
    def forward(self, tokens, attn_scale=None):
        """
        tokens: [B, H, W, D] æˆ– [B, N, D]
        å¦‚æœ use_spherical_attn=Trueï¼Œtokens åº”è¯¥æ˜¯ [B, H, W, D]
        å¦åˆ™ tokens åº”è¯¥æ˜¯ [B, N, D]
        attn_scale: Attention scale factor for dynamic scaling.
        """
        original_shape = tokens.shape
        B = tokens.shape[0]
        
        # å¤„ç†è¾“å…¥å½¢çŠ¶
        if len(original_shape) == 3:
            # [B, N, D] -> [B, H, W, D] æˆ–ä¿æŒ [B, N, D]
            B, N, D = tokens.shape
            if self.use_spherical_attn:
                H, W = self.in_shape
                if N != H * W:
                    raise ValueError(f"Tokenæ•°é‡ {N} ä¸å½¢çŠ¶ {self.in_shape} ä¸åŒ¹é… (éœ€è¦ {H*W})")
                tokens = tokens.reshape(B, H, W, D)
        elif len(original_shape) == 4:
            # [B, H, W, D] - å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
            B, H, W, D = tokens.shape
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥å½¢çŠ¶: {original_shape}")
        
        # åœ¨å½¢çŠ¶å¤„ç†å®Œæˆåå…‹éš†ï¼Œç¡®ä¿ x ä¸ tokens å½¢çŠ¶ä¸€è‡´
        x = tokens.clone()
        
        # ä½¿ç”¨ä¼ å…¥çš„ attn_scaleï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if attn_scale is None:
            attn_scale = self.attn_scale.item() if hasattr(self, 'attn_scale') else 0.1
        else:
            attn_scale = float(attn_scale)
        
        # åº”ç”¨å¤šå±‚å¤„ç†
        if self.use_spherical_attn:
            # çƒé¢æ³¨æ„åŠ›æ¨¡å¼ï¼šéœ€è¦è½¬æ¢ä¸º [B, C, H, W] æ ¼å¼
            attn_out = None  # åˆå§‹åŒ– attn_out
            for i, layer in enumerate(self.sparse_attns):
                # è½¬æ¢ä¸º [B, C, H, W] æ ¼å¼
                tokens_4d = tokens.permute(0, 3, 1, 2)  # [B, D, H, W]
                
                # æ³¨æ„åŠ›å±‚
                # LayerNorm éœ€è¦æœ€åä¸€ä¸ªç»´åº¦æ˜¯ dimï¼Œæ‰€ä»¥å…ˆè½¬æ¢ä¸º [B, H, W, D]
                tokens_4d = tokens_4d.permute(0, 2, 3, 1)  # [B, H, W, D]
                tokens_4d = layer["norm1"](tokens_4d)
                tokens_4d = tokens_4d.permute(0, 3, 1, 2)  # [B, D, H, W] è½¬å›é€šé“ä¼˜å…ˆæ ¼å¼
                # gate_attn = torch.sigmoid(self.gate_attn[i])
                # breakpoint()
                # åº”ç”¨ attn_scale ç¼©æ”¾ attention è¾“å‡º
                attn_out = layer["attn"](tokens_4d)  # [B, D, H, W]
                tokens_4d = tokens_4d + attn_scale * attn_out

                # MLPå±‚ï¼ˆéœ€è¦è½¬æ¢å› [B, H, W, D]ï¼‰
                tokens_4d = tokens_4d.permute(0, 2, 3, 1)  # [B, H, W, D]

                if hasattr(layer, "ffn"):
                    # gate_ffn = torch.sigmoid(self.gate_ffn[i])
                    # tokens_4d = tokens_4d + gate_ffn * layer["ffn"](layer["norm2"](tokens_4d))
                    tokens_4d = tokens_4d + layer["ffn"](layer["norm2"](tokens_4d))
                
                tokens = tokens_4d  # [B, H, W, D]
            
            # åœ¨çƒé¢æ³¨æ„åŠ›æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ ffn å’Œ attn_out è¿›è¡Œèåˆ
            out_mlp = self.ffn(x)  # x æ˜¯ [B, H, W, D]ï¼Œout_mlp ä¹Ÿæ˜¯ [B, H, W, D]
            # attn_out æ˜¯ [B, D, H, W]ï¼Œéœ€è¦è½¬æ¢ä¸º [B, H, W, D]
            attn_out_4d = attn_out.permute(0, 2, 3, 1)  # [B, H, W, D]
            
            # æ ‡å‡†åŒ–åå†èåˆï¼ˆæœ‰åŠ©äºç¨³å®šè®­ç»ƒå’Œå¹³è¡¡ä¸¤ä¸ªåˆ†æ”¯çš„è´¡çŒ®ï¼‰
            out_mlp_norm = self.fusion_norm_mlp(out_mlp)
            attn_out_norm = self.fusion_norm_attn(attn_out_4d)
            tokens = out_mlp_norm + attn_scale * attn_out_norm
        else:
            # æ ‡å‡†æ³¨æ„åŠ›æ¨¡å¼ï¼šéœ€è¦ [B, N, D] æ ¼å¼
            # å¦‚æœtokensæ˜¯4ç»´çš„ï¼Œå…ˆreshapeæˆ3ç»´
            if len(tokens.shape) == 4:
                B, H, W, D = tokens.shape
                tokens = tokens.reshape(B, H * W, D)  # [B, H*W, D]
                # åŒæ—¶æ›´æ–° x çš„å½¢çŠ¶
                x = x.reshape(B, H * W, D)
            
            B, N, D = tokens.shape
            
            attn_out = None  # åˆå§‹åŒ– attn_out
            for layer in self.sparse_attns:
                tokens = layer["norm1"](tokens)
                
                # ä½¿ç”¨chunkå¤„ç†æ³¨æ„åŠ›è®¡ç®—ä»¥èŠ‚çœå†…å­˜
                attn_outputs = []
                # for i in range(0, N, self.chunk_size):
                # chunk_tokens = tokens[:, i:i + self.chunk_size]  # [B, chunk_size, D]
                attn_out, _ = layer["attn"](tokens, tokens, tokens)  # [B, N, D]
                # attn_outputs.append(attn_out)
                # attn_out = torch.cat(attn_outputs, dim=1)  # [B, N, D]
                
                tokens = tokens + attn_out
                tokens = layer["norm2"](tokens)
                tokens = tokens + layer["mlp"](tokens)
            
            # åœ¨æ ‡å‡†æ³¨æ„åŠ›æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ ffn å’Œ attn_out è¿›è¡Œèåˆ
            out_mlp = self.ffn(x)  # x æ˜¯ [B, N, D]ï¼Œout_mlp ä¹Ÿæ˜¯ [B, N, D]
            # attn_out å·²ç»æ˜¯ [B, N, D]ï¼Œä¸éœ€è¦ permute
            
            # æ ‡å‡†åŒ–åå†èåˆï¼ˆæœ‰åŠ©äºç¨³å®šè®­ç»ƒå’Œå¹³è¡¡ä¸¤ä¸ªåˆ†æ”¯çš„è´¡çŒ®ï¼‰
            out_mlp_norm = self.fusion_norm_mlp(out_mlp)
            attn_out_norm = self.fusion_norm_attn(attn_out)
            tokens = out_mlp_norm + attn_scale * attn_out_norm
            
        # æŠ•å½±åˆ°è¾“å‡ºç»´åº¦
        if self.out_dim != self.dim:
            tokens = self.proj_out(tokens) # mlp
        
        # ä¿æŒè¾“å‡ºå½¢çŠ¶ä¸è¾“å…¥å½¢çŠ¶ä¸€è‡´
        if not self.use_spherical_attn:
            # æ ‡å‡†æ³¨æ„åŠ›æ¨¡å¼
            if len(original_shape) == 4:
                # å¦‚æœåŸå§‹è¾“å…¥æ˜¯4ç»´çš„ï¼Œéœ€è¦reshapeå›4ç»´
                B, H, W, _ = original_shape
                tokens = tokens.reshape(B, H, W, self.out_dim)  # [B, H, W, out_dim]
            # å¦‚æœåŸå§‹è¾“å…¥æ˜¯3ç»´çš„ï¼Œtokenså·²ç»æ˜¯ [B, N, out_dim]ï¼Œä¿æŒä¸å˜
        elif len(original_shape) == 3 and self.use_spherical_attn:
            # å¦‚æœè¾“å…¥æ˜¯ [B, N, D] ä½†ä½¿ç”¨äº†çƒé¢æ³¨æ„åŠ›ï¼Œéœ€è¦è½¬æ¢å› [B, N, out_dim]
            B, H, W, D = tokens.shape
            tokens = tokens.reshape(B, H * W, D)
        # å¦‚æœè¾“å…¥æ˜¯ [B, H, W, D]ï¼Œè¾“å‡ºä¹Ÿä¿æŒ [B, H, W, out_dim]
        
        return tokens

class GlobalOperatorToken(nn.Module):
    """
    ä»çƒé¢ operator latent ä¸­æŠ½å–å…¨å±€æ¡ä»¶ token
    U: [B, L, D] -> z_g: [B, 1, D]
    ä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›å‡å°‘è®¡ç®—é‡
    """
    def __init__(
        self,
        dim,
        num_heads=8,
        dropout=0.0,
        use_residual=True,
        num_query=1,
    ):
        super().__init__()

        self.num_query = num_query
        self.query = nn.Parameter(torch.randn(1, num_query, dim))

        self.attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True,
        )

        self.norm_kv = nn.LayerNorm(dim)
        self.norm_out = nn.LayerNorm(dim)

        self.use_residual = use_residual

    def forward(self, U):
        """
        U: [B, L, D]
        return:
            z_g: [B, 1, D]
        """
        B, L, D = U.shape

        q = self.query.expand(B, self.num_query, -1)      # [B,num_query,D]
        kv = self.norm_kv(U)

        z_g, _ = self.attn(q, kv, kv)         # [B,1,D]

        if self.use_residual:
            z_g = z_g + q

        return self.norm_out(z_g)

class GlobalOperatorReadout(nn.Module):
    """
    è¾“å‡º:
      - global token
      - augmented operator tokens
    """
    def __init__(self, dim, num_heads=8, dropout=0.0):
        super().__init__()
        self.readout = GlobalOperatorToken(
            dim=dim,
            num_heads=num_heads,
            dropout=dropout,
        )

    def forward(self, U, return_augmented=True):
        """
        U: [B, L, D]
        """
        z_g = self.readout(U)  # [B,1,D]

        if return_augmented:
            U_aug = torch.cat([z_g, U], dim=1)  # [B, L+1, D]
            return z_g, U_aug

        return z_g

class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        # x = self.drop(x)
        x = self.fc2(x)
        # x = self.drop(x)
        return x
    
class FourierEmbs(nn.Module):
    def __init__(self, embed_scale, embed_dim):
        super(FourierEmbs, self).__init__()
        self.embed_scale = embed_scale
        self.embed_dim = embed_dim
        self.kernel = nn.Parameter(torch.randn(2, self.embed_dim // 2))

    def forward(self, x):
        
        # åº”ç”¨å‚…é‡Œå¶å˜æ¢
        x_proj = x @ self.kernel  # [N, 2] @ [2, 512] -> [N, 512]
        y = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)  # [N, 1024]
        return y

class myMlp(nn.Module):
    def __init__(self, num_layers, hidden_dim, out_dim, layer_norm_eps=1e-5):
        super(myMlp, self).__init__()
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.out_dim = out_dim
        self.layer_norm_eps = layer_norm_eps
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.hidden_dim, self.hidden_dim),
                nn.GELU(),
                nn.LayerNorm(self.hidden_dim, eps=self.layer_norm_eps)
            ) for _ in range(self.num_layers)
        ])
        self.output_layer = nn.Linear(self.hidden_dim, self.out_dim)

    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)
        x = self.output_layer(x)
        return x

class ClimaX(nn.Module):
    """Implements the ClimaX model as described in the paper,
    https://arxiv.org/abs/2301.10343

    Args:
        default_vars (list): list of default variables to be used for training
        img_size (list): image size of the input data
        patch_size (int): patch size of the input data
        embed_dim (int): embedding dimension
        depth (int): number of transformer layers
        decoder_depth (int): number of decoder layers
        num_heads (int): number of attention heads
        mlp_ratio (float): ratio of mlp hidden dimension to embedding dimension
        drop_path (float): stochastic depth rate
        drop_rate (float): dropout rate
        parallel_patch_embed (bool): whether to use parallel patch embedding
    """

    def __init__(
        self, 
        default_vars,
        # encoder,
        time_range=4,
        img_size=[32, 64],
        patch_size=2,
        embed_dim=1024,
        encoder_depth=8,
        fuse_decoder_depth=2,
        decoder_depth=2,
        num_heads=16,
        mlp_ratio=4.0,
        drop_path=0.1,
        drop_rate=0.1,
        parallel_patch_embed=False,
        low_gird=(32, 64),high_gird=(64,128), pde_weight=0.0001, fourier_weight=1.0,latent_dim=1024, emb_dim=1024, dec_emb_dim=768, dec_num_heads=16, dec_depth=1, num_mlp_layers=1, out_dim=5, eps=1e5, layer_norm_eps=1e-5, embedding_type="latlon", chunk_size=10, num_global_operator_token=16, theta_cutoff=None, use_spherical_attn=True
    ):
        super().__init__()

    # Feature Extracer
    # --------------------------------------------------------------------------
       # TODO: remove time_history parameter
        self.img_size = img_size
        self.patch_size = patch_size
        self.default_vars = default_vars
        self.parallel_patch_embed = parallel_patch_embed
        self.time_range = time_range
        
        self.embedmlp = MLP(in_features=emb_dim, out_features=dec_emb_dim)
       
        self.high_lamda = 0.0001
        
        self.low_gird = low_gird
        self.high_grid = high_gird
        self.latent_dim = latent_dim
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.dec_emb_dim = dec_emb_dim
        self.dec_num_heads = dec_num_heads
        self.dec_depth = dec_depth
        self.num_mlp_layers = num_mlp_layers
        self.mlp_ratio = mlp_ratio
        self.out_dim = out_dim
        self.eps = eps
        self.layer_norm_eps = layer_norm_eps
        self.embedding_type = embedding_type
        self.chunk_size = chunk_size  # Chunk size for processing query points to reduce memory
        
        self.coord_norm = nn.LayerNorm(self.dec_emb_dim, eps=self.layer_norm_eps)
       
        self.global_operator_token = GlobalOperatorToken(
            dim=self.dec_emb_dim,
            num_heads=self.dec_num_heads,
            dropout=drop_rate,
            use_residual=True,
            num_query=num_global_operator_token,
        )
        
        # Spherical set operator for query points and operator tokens interaction
        self.spherical_operator = SphericalCrossOperatorS2(
            dim=self.dec_emb_dim,
            num_heads=self.dec_num_heads,
            mlp_ratio=self.mlp_ratio,
            in_shape=(self.low_gird[0].shape[0], self.low_gird[1].shape[0]),  # (H, W) - ç¡®ä¿æ˜¯å…ƒç»„
            out_shape=(self.high_grid[0].shape[0], self.high_grid[1].shape[0]),  # (H, W) - ç¡®ä¿æ˜¯å…ƒç»„
            theta_cutoff=theta_cutoff, 
            use_spherical_attn=use_spherical_attn,
        )
        
        self.spatial_norm = SpatialNorm(eps=self.layer_norm_eps)
        self.fusion_norm = nn.LayerNorm(self.dec_emb_dim, eps=self.layer_norm_eps)

        # ä½¿ç”¨ SphericalSparseOperator æ›¿ä»£ head
        # åˆ›å»ºè‡ªå®šä¹‰çš„ norm_layer å‡½æ•°
        def create_norm_layer(dim):
            return nn.LayerNorm(dim, eps=self.layer_norm_eps)
        
        # self.head = SphericalSparseOperator(
        #     dim=self.dec_emb_dim,
        #     num_heads=self.dec_num_heads,
        #     out_dim=self.out_dim,
        #     in_shape=(self.high_grid[0].shape[0], self.high_grid[1].shape[0]),  # (H, W) - ç¡®ä¿æ˜¯å…ƒç»„
        #     out_shape=(self.high_grid[0].shape[0], self.high_grid[1].shape[0]),  # (H, W) - ç¡®ä¿æ˜¯å…ƒç»„
        #     mlp_ratio=self.mlp_ratio,
        #     num_layers=self.dec_depth,
        #     drop_path=drop_path,
        #     norm_layer=create_norm_layer,
        #     chunk_size=self.chunk_size,
        # )

        self.head = myMlp(num_layers=self.num_mlp_layers, hidden_dim=self.dec_emb_dim, out_dim=self.out_dim, layer_norm_eps=self.layer_norm_eps)

        # Create low grid and latents
        n_x, n_y = low_gird[0], low_gird[1]
        xx, yy = torch.meshgrid(n_x, n_y, indexing="ij")
        self.grid = torch.hstack([xx.flatten()[:, None], yy.flatten()[:, None]])
        self.latents = nn.Parameter(torch.randn(len(n_x) * len(n_y), self.latent_dim))

        #high
        n_x_high, n_y_high = high_gird[0], high_gird[1]
        xx_high, yy_high = torch.meshgrid(n_x_high,n_y_high, indexing="ij")
        self.grid_high = torch.hstack([xx_high.flatten()[:, None], yy_high.flatten()[:, None]])
    
        self.fourier_encoding = FourierEmbs(embed_scale=2 * np.pi, embed_dim=self.latent_dim)
        self.coord_fourier_grid_proj = MLP(self.latent_dim * 2, self.dec_emb_dim*2 ,self.dec_emb_dim)
        
        # æµ‹åº¦è·ç¦»å‚æ•°
        self.use_geodesic_distance = True  # æ˜¯å¦ä½¿ç”¨æµ‹åº¦è·ç¦»
        self.sphere_radius = 1.0  # çƒé¢åŠå¾„ï¼ˆå•ä½çƒé¢ï¼‰
        self.coords_in_degrees = True  # åæ ‡å•ä½æ˜¯å¦ä¸ºåº¦æ•°ï¼ˆFalse è¡¨ç¤ºå¼§åº¦ï¼‰
       
        # --------------------------------------------------------------------------
    
    def _compute_position_encoding(self, query_coords, reference_grid=None, use_geodesic=True, sphere_radius=1.0, coords_in_degrees=True):
        """
        é€šç”¨çš„ä½ç½®ç¼–ç è®¡ç®—å‡½æ•°
        
        Args:
            query_coords: [N, 2] æŸ¥è¯¢åæ ‡ï¼ˆéœ€è¦ç¼–ç çš„åæ ‡ç‚¹ï¼‰ï¼Œå•ä½ï¼šå¼§åº¦ (lat, lon)
            reference_grid: [M, 2] å‚è€ƒç½‘æ ¼ï¼ˆç”¨äºè®¡ç®—è·ç¦»å’ŒåŠ æƒï¼‰ï¼Œå•ä½ï¼šå¼§åº¦ (lat, lon)
                           å¦‚æœä¸º None åˆ™ä½¿ç”¨ query_coords è‡ªèº«
            use_geodesic: bool, æ˜¯å¦ä½¿ç”¨æµ‹åº¦è·ç¦»ï¼ˆgeodesic distanceï¼‰ï¼Œé»˜è®¤ True
            sphere_radius: float, çƒé¢åŠå¾„ï¼Œç”¨äºæµ‹åº¦è·ç¦»è®¡ç®—ï¼Œé»˜è®¤ 1.0
            coords_in_degrees: bool, åæ ‡å•ä½æ˜¯å¦ä¸ºåº¦æ•°ï¼ˆFalse è¡¨ç¤ºå¼§åº¦ï¼‰
        Returns:
            encoded: [N, dec_emb_dim] ç¼–ç åçš„ä½ç½®ç‰¹å¾
        """
        device = query_coords.device if hasattr(query_coords, 'device') else self.latents.device
        query_coords = query_coords.float()  # [N, 2]
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå‚è€ƒç½‘æ ¼ï¼Œä½¿ç”¨æŸ¥è¯¢åæ ‡è‡ªèº«ï¼ˆç”¨äºä½åˆ†è¾¨ç‡è‡ªç¼–ç ï¼‰
        if reference_grid is None:
            reference_grid = query_coords
        
        reference_grid = reference_grid.to(device).float()  # [M, 2]
        latents = self.latents.to(device)  # [M, latent_dim]
        
        # 1. Fourier ç¼–ç 
        fourier_embed = self.fourier_encoding(query_coords)  # [N, latent_dim]
        
        # 2. è®¡ç®—è·ç¦»ï¼šquery_coords åˆ° reference_grid çš„è·ç¦»
        if use_geodesic:
            # ä½¿ç”¨æµ‹åº¦è·ç¦»ï¼ˆgeodesic distanceï¼‰
            d2 = compute_geodesic_distance_squared(
                query_coords, 
                reference_grid, 
                radius=sphere_radius,
                coords_in_degrees=self.coords_in_degrees
            )  # [N, M]
        else:
            # ä½¿ç”¨æ¬§å¼è·ç¦»ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
            d2 = ((query_coords[:, None, :] - reference_grid[None, :, :]) ** 2).sum(dim=2)  # [N, M]
        
        # 3. ä½¿ç”¨ softmax æƒé‡ï¼ˆè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        w = torch.softmax(-self.eps * d2, dim=-1)  # [N, M]
        
        # 4. ä½¿ç”¨æƒé‡åŠ æƒ latents
        weighted_latents = (latents.T @ w.T).T  # [latent_dim, M] @ [M, N] -> [N, latent_dim]
        
        # 5. æ‹¼æ¥åŠ æƒåçš„ latents å’Œ Fourier ç¼–ç 
        encoded_input = torch.cat([weighted_latents, fourier_embed], dim=-1)  # [N, latent_dim * 2]
        
        # 6. æ˜ å°„åˆ°è§£ç å™¨ç»´åº¦
        encoded = self.coord_fourier_grid_proj(encoded_input)  # [N, dec_emb_dim]
        encoded = self.coord_norm(encoded)
        
        return encoded
    
    def get_low_res_pe(self, b, device):
        """
        ä½åˆ†è¾¨ç‡ä½ç½®ç¼–ç ï¼Œä½¿ç”¨ä¸é«˜åˆ†è¾¨ç‡ç›¸åŒçš„ç¼–ç æ–¹å¼
        è®¡ç®—ä½åˆ†è¾¨ç‡ç½‘æ ¼ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨ softmax æƒé‡åŠ æƒ latents
        """
        # self.grid æ˜¯åˆå§‹åŒ–æ—¶ä¿å­˜çš„ [low_h * low_w, 2]
        low_grid = self.grid.to(device).float()  # [L, 2]
        
        # ä½¿ç”¨é€šç”¨ç¼–ç å‡½æ•°ï¼Œreference_grid=None è¡¨ç¤ºä½¿ç”¨è‡ªèº«ä½œä¸ºå‚è€ƒï¼ˆè‡ªç¼–ç ï¼‰
        low_pe = self._compute_position_encoding(
            low_grid, 
            reference_grid=None,
            use_geodesic=self.use_geodesic_distance,
            sphere_radius=self.sphere_radius
        )  # [L, dec_emb_dim]
        
        return low_pe.unsqueeze(0).expand(b, -1, -1)  # [B, L, dec_emb_dim]

    def coord_encoding_Fourier(self, b, coords):
        """
        é«˜åˆ†è¾¨ç‡ä½ç½®ç¼–ç 
        è®¡ç®—é«˜åˆ†è¾¨ç‡åæ ‡åˆ°ä½åˆ†è¾¨ç‡ç½‘æ ¼çš„è·ç¦»ï¼Œå¹¶ä½¿ç”¨ softmax æƒé‡åŠ æƒ latents
        """
        # coords.shape [H*W, 2] æˆ– [H, W, 2]
        coords = coords.reshape(-1, 2).to(self.latents.device).float()  # [H*W, 2]
        self.grid = self.grid.to(self.latents.device)
        
        # ä½¿ç”¨é€šç”¨ç¼–ç å‡½æ•°ï¼Œreference_grid=self.grid è¡¨ç¤ºä½¿ç”¨ä½åˆ†è¾¨ç‡ç½‘æ ¼ä½œä¸ºå‚è€ƒ
        coords_encoded = self._compute_position_encoding(
            coords, 
            reference_grid=self.grid,
            use_geodesic=self.use_geodesic_distance,
            sphere_radius=self.sphere_radius,
            coords_in_degrees=self.coords_in_degrees,
        )  # [H*W, dec_emb_dim]
        
        # Reshape å›ç©ºé—´ç»´åº¦
        coords_encoded = coords_encoded.unsqueeze(0).expand(b, -1, -1)  # [B, H*W, dec_emb_dim]
        coords_encoded = coords_encoded.reshape(b, len(self.high_grid[0]), len(self.high_grid[1]), -1)  # [B, H, W, dec_emb_dim]
        
        return coords_encoded
        
    #---------------------------------------------------------------------------------
    # Operator: Spherical set operator for query points and operator tokens interaction
    def Operator(self, x, coords):
        # x: [B, L, D]
        # coords: [B, H, W, D]

        B, H, W, D = coords.shape
        N = H * W
        coords = coords.reshape(B, N, D)

        # prepend global token
        # z_g = self.global_operator_token(x)     # [B,1,D]
        # tokens = torch.cat([z_g, x], dim=1)     # [B,num_global_operator_token + L,D]
        # coords = coords + z_g.unsqueeze(1)
        coords_spatial = self.spherical_operator(coords, x)    # [B,N,D]

        coords_normalized = self.fusion_norm(coords_spatial)
        # Reshape to [B, H, W, D] for SphericalSparseOperator
        coords_spatial = coords_normalized.reshape(B, H, W, self.dec_emb_dim)
        # Spatial normalization
        coords_spatial = self.spatial_norm(coords_spatial)
        # ä½¿ç”¨ SphericalSparseOperator è¿›è¡Œçƒé¢ç¨€ç–æ³¨æ„åŠ›å’Œç»´åº¦å˜æ¢
        coords_out = self.head(coords_spatial)  # [B, H, W, out_dim] 
        return coords_out

       
#---------------------------------------------------------------------------------


    def forward(self, embed_U, x, y, res, out_variables, metric, lat):
        """Forward pass through the model.

        Args:
            x: `[B, Vi, H, W]` shape. Input weather/climate variables
            y: `[B, Vo, H, W]` shape. Target weather/climate variables
            lead_times: `[B]` shape. Forecasting lead times of each element of the batch.
            attn_scale: Attention scale factor for dynamic scaling.

        Returns:
            loss (list): Different metrics.
            preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.
        """
        # x = x[:,0,:]

        embed_U = embed_U.squeeze(1)
        embed_U = self.embedmlp(embed_U)

        coords_high = self.coord_encoding_Fourier(x.shape[0], self.grid_high)  #[B , H*W , L]
        low_pe = self.get_low_res_pe(x.shape[0], x.device)

        token_low = embed_U + low_pe 

        preds = self.Operator(token_low, coords_high).permute(0,3,1,2)
        preds = preds + res
        
        if metric is None:
            loss = None
        else:
            loss = [m(preds, y, out_variables, lat) for m in metric]

        return loss, preds

    def evaluate(self, embed_U, x, y, res, out_variables, transform, metrics, lat, clim, log_postfix):
        _, preds = self.forward(embed_U, x, y, res, out_variables, metric=None, lat=lat)
        num_vars = len(out_variables)
        return [m(preds[:, :num_vars], y[:, :num_vars], transform, out_variables, lat, clim, log_postfix) for m in metrics]
